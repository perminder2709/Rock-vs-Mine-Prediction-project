{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d057b-e646-447e-a5eb-8679d3b2947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is deep learning, and how is it connected to artificial intelligence?\n",
    "Deep learning is a subset of machine learning in artificial intelligence (AI) that uses neural networks with many layers (deep neural networks) to model complex patterns in data. It powers applications like image recognition, speech processing, and autonomous systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d21f2-79b0-479f-aeba-7055cd9e325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a neural network, and what are the different types of neural networks?\n",
    "A neural network is a computational model inspired by the human brain, composed of layers of interconnected nodes (neurons).\n",
    "Types include:\n",
    "\n",
    "Feedforward Neural Networks (FNN)\n",
    "\n",
    "Convolutional Neural Networks (CNN)\n",
    "\n",
    "Recurrent Neural Networks (RNN)\n",
    "\n",
    "Generative Adversarial Networks (GAN)\n",
    "\n",
    "Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27422542-c534-493c-807d-00049bc047f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is an activation function, and why is it essential in neural networks?\n",
    "Activation functions introduce non-linearity into the model, enabling it to learn complex patterns. Without them, the network behaves like a linear model regardless of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b57c58-7741-4295-8c7e-f184fb258dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could you list some common activation functions used in neural networks?\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "Tanh\n",
    "\n",
    "ReLU (Rectified Linear Unit)\n",
    "\n",
    "Leaky ReLU\n",
    "\n",
    "Softmax (usually for output layer in classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b7c48-3763-41e1-822c-fb2e07fcea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a multilayer neural network?\n",
    "A multilayer neural network consists of an input layer, one or more hidden layers, and an output layer. These deep networks can model more complex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb619e-3e6a-4c85-b98b-428ec728d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a loss function, and why is it crucial for neural network training?\n",
    "A loss function measures how far the network's predictions are from the actual targets. It guides the optimization process during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d302e84-5ed0-4639-8997-c67aebd4a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are some common types of loss functions?\n",
    "\n",
    "Mean Squared Error (MSE)\n",
    "\n",
    "Cross-Entropy Loss\n",
    "\n",
    "Hinge Loss\n",
    "\n",
    "Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18514896-32b2-4172-950e-1e77ae673abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does a neural network learn?\n",
    "Through a process called training, using forward propagation to make predictions and backpropagation to adjust weights based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8c322-9baf-4d82-85de-a2ae1e3a69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is an optimizer in neural networks, and why is it necessary?\n",
    "An optimizer updates the model's weights to minimize the loss function. It determines how efficiently and effectively a model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ce77c-c70d-4d13-8692-1b64b14354da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could you briefly describe some common optimizers?\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Adam (Adaptive Moment Estimation)\n",
    "\n",
    "RMSprop\n",
    "\n",
    "Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c423bc8-10fd-4d4f-98ea-3c3abbb9d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can you explain forward and backward propagation in a neural network?\n",
    "\n",
    "Forward Propagation: Input is passed through the network to produce an output.\n",
    "\n",
    "Backward Propagation: The loss is propagated back through the network to compute gradients, which are used to update weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd7ee6-80c5-44ff-92df-8f78879f6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is weight initialization, and how does it impact training?\n",
    "Weight initialization sets initial values for weights. Good initialization helps speed up convergence and avoid issues like vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4ca52-a4db-4b98-b684-e1cdc2365afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the vanishing gradient problem in deep learning?\n",
    "In deep networks, gradients can become very small during backpropagation, slowing or halting learning in early layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675923b-adf3-4397-bf47-ce108162f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the exploding gradient problem?\n",
    "The opposite of vanishing gradients: gradients grow exponentially, causing unstable updates and possibly numerical overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f972401-6c94-4cb6-a22d-eb003faed020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you create a simple perceptron for basic binary classification?\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))  # Basic Perceptron\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de33be-4d3f-41a4-8952-564d10a5aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can you build a neural network with one hidden layer using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))            # Output layer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b884b7d-22c5-4e8e-8b82-19ec105dec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu', kernel_initializer=GlorotUniform()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663c626-0c4a-43b8-b2e6-4ff1b9541db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you apply different activation functions in a neural network in Keras?\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=10))\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaabd53-91fe-4214-8610-f980cb91e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you add dropout to a neural network model to prevent overfitting?\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f686a3-3958-4936-ab9f-588255137fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you manually implement forward propagation in a simple neural network?\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Weights and biases\n",
    "W = np.array([[0.5], [0.3]])\n",
    "b = 0.1\n",
    "X = np.array([[1.0], [2.0]])\n",
    "\n",
    "# Forward propagation\n",
    "Z = np.dot(W.T, X) + b\n",
    "A = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7f302-430a-4792-930c-b3871f598766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you add batch normalization to a neural network model in Keras?\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182c30a-89dd-4703-9806-00735d78b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you visualize the training process with accuracy and loss curves?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.plot(history.history['accuracy'], label='acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338a917-7bea-4a6c-95a8-ad5f1f494dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(clipvalue=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de84224-7fdb-4ae6-96e6-8e4ebf251bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you create a custom loss function in Keras?\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989628c-fc22-4dc3-be8e-1699d8a740dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you visualize the structure of a neural network model in Keras?\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57a574-0d0a-43d9-a08c-79d25c7b0ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
