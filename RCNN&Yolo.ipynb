{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74779981-1199-4947-8fc5-5853c8e9379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the main purpose of RCNN in object detection?\n",
    "Answer: RCNN (Regions with CNN features) aims to accurately detect and classify objects within an image by combining region proposals with convolutional neural networks to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00239f9-e943-4ca4-8bf8-76d09b292430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the difference between Fast RCNN and Faster RCNN?\n",
    "Answer: Fast RCNN uses selective search to propose regions and processes them using a single CNN. Faster RCNN replaces selective search with a Region Proposal Network (RPN), making it significantly faster and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed52ffd-6d16-455b-be8d-dbe84685b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does YOLO handle object detection in real-time?\n",
    "Answer: YOLO (You Only Look Once) processes the entire image with a single neural network in one pass, predicting bounding boxes and class probabilities simultaneously, enabling real-time detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a3e4c-3e48-418b-8142-14f46b708451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.\n",
    "Answer: RPNs are neural networks that propose regions likely to contain objects. These proposals are then passed to a classifier and regressor in the Faster RCNN pipeline for final detection and localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a214d-67fc-489c-9546-c0e5ade5d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does YOLOv9 improve upon its predecessors?\n",
    "Answer: YOLOv9 introduces architectural enhancements like better feature extraction, improved attention mechanisms, and optimized loss functions, leading to higher accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa516c6-01b3-4bb1-904f-0a1a1acb0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What role does non-max suppression play in YOLO object detection?\n",
    "Answer: Non-max suppression removes overlapping bounding boxes by keeping only the one with the highest confidence score, reducing duplicate detections of the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d8629-8e1e-4193-9d27-64aa31268915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the data preparation process for training YOLOv9.\n",
    "Answer: It involves image resizing, normalization, data augmentation (flipping, cropping), and converting annotations into a YOLO-compatible format (class, x, y, width, height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9609-38b9-40d5-9af1-d0af102e5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the significance of anchor boxes in object detection models like YOLOv9?\n",
    "Answer: Anchor boxes are predefined shapes and sizes that help detect objects of various scales and aspect ratios by improving bounding box prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d55bb-781c-4091-b360-0926b58990d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the key difference between YOLO and R-CNN architectures?\n",
    "Answer: YOLO is a single-stage detector that processes the entire image at once, while R-CNN is a two-stage detector that first proposes regions and then classifies them, making YOLO faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebeaaf-8535-4676-b5ef-9d0c520d32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why is Faster RCNN considered faster than Fast RCNN?\n",
    "Answer: Because Faster RCNN includes an RPN that shares computation with the main detection network, eliminating the need for external region proposal methods like selective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca462ec-d7d7-4899-bf58-5e0490312a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the role of selective search in RCNN?\n",
    "Answer: Selective search is used to generate region proposals in RCNN, which are then passed through CNNs for feature extraction and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18123f91-4056-4412-9df7-a96e52357c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does YOLOv9 handle multiple classes in object detection?\n",
    "Answer: YOLOv9 predicts a set of class probabilities for each bounding box, allowing it to identify and classify multiple objects across different classes in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472bc4a-b027-436e-b31f-b2465870c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the key differences between YOLOv3 and YOLOv9?\n",
    "Answer: YOLOv9 offers better performance due to advancements in backbone networks, loss functions, attention mechanisms, and training strategies compared to YOLOv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d602cb6-9580-424f-935c-8375b46b43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How is the loss function calculated in Faster RCNN?\n",
    "Answer: It combines classification loss (e.g., cross-entropy for object categories) and regression loss (e.g., smooth L1 loss for bounding box coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49160d86-8c5f-40fe-9efd-3fb9ebd434bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain how YOLOv9 improves speed compared to earlier versions.\n",
    "Answer: Through architecture optimization, more efficient backbone networks, and lighter heads, YOLOv9 reduces computational load while maintaining or improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efc813-412f-4b91-8a19-c59f1d09aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are some challenges faced in training YOLOv9?\n",
    "Answer: Challenges include data imbalance, detecting small or overlapping objects, tuning hyperparameters, and ensuring real-time performance without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f880c-d21a-4905-a2a0-6006779bf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does the YOLOv9 architecture handle large and small object detection?\n",
    "Answer: It uses feature pyramid networks and multi-scale detection layers to handle objects at various scales effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e3309-6208-4608-80cb-c23d099e8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the significance of fine-tuning in YOLO?\n",
    "Answer: Fine-tuning pre-trained YOLO models on specific datasets improves performance and allows the model to adapt to new object classes or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f9b4b-0a29-4b15-bfd4-b567dfc81174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the concept of bounding box regression in Faster RCNN?\n",
    "Answer: It involves predicting the precise coordinates of an objectâ€™s bounding box based on anchor boxes and refining them through regression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4a224-5d4d-4050-8b2e-2c6ad8ed3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe how transfer learning is used in YOLO.\n",
    "Answer: YOLO models pre-trained on large datasets (e.g., COCO) are fine-tuned on custom datasets, reducing training time and improving performance on new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21023054-64e2-4289-bab0-4ba334b89543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the role of the backbone network in object detection models like YOLOv9?\n",
    "Answer: The backbone extracts features from input images. Stronger backbones (e.g., CSPDarknet, EfficientNet) lead to better accuracy in detecting objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511ae8a-5b95-4e7b-8693-92504dd33d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does YOLO handle overlapping objects?\n",
    "Answer: YOLO handles overlaps using anchor boxes and non-max suppression to predict the most accurate bounding box per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c51f77-9849-4aad-9c7c-7fc15520ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the importance of data augmentation in object detection?\n",
    "Answer: It increases data diversity, improves generalization, and helps models become robust to variations in scale, rotation, and lighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d6b57-1963-4146-a484-c54cb197cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How is performance evaluated in YOLO-based object detection?\n",
    "Answer: Using metrics like mAP (mean Average Precision), IoU (Intersection over Union), precision, recall, and FPS (frames per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269f458-7af1-4992-9fbb-a82b5b1fc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do the computational requirements of Faster RCNN compare to those of YOLO?\n",
    "Answer: Faster RCNN is more computationally intensive due to its two-stage nature, while YOLO is optimized for speed and efficiency with real-time capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39840c16-3669-4e43-9362-70b422fe1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What role do convolutional layers play in object detection with RCNN?\n",
    "Answer: They extract hierarchical features from images that help distinguish between object classes and localize them within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32835bba-e712-409a-ae11-1ea6a6e9b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does the loss function in YOLO differ from other object detection models?\n",
    "Answer: YOLO combines objectness loss, classification loss, and localization loss into a single loss function, making it end-to-end trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6da2c5-72d7-43af-9ec9-d355a5dd0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the key advantages of using YOLO for real-time object detection?\n",
    "Answer: Speed, end-to-end training, high FPS, ability to detect multiple objects in one pass, and good balance between accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff113e98-3969-442d-80ba-8f7b847a81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does Faster RCNN handle the trade-off between accuracy and speed?\n",
    "Answer: It prioritizes accuracy through precise region proposals and deeper networks but sacrifices real-time speed due to its two-stage process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5a2ff-19ef-4163-8c57-628700e1844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ?\n",
    "Answer: In both models, the backbone extracts features. YOLO uses lightweight, speed-optimized backbones (like CSPDarknet), while Faster RCNN may use heavier, more accurate ones (like ResNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968e8876-a2c2-4bd6-87a1-83fe5dd9ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOv9)?\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')  # Replace with yolov9 if available\n",
    "results = model('path/to/image.jpg')  # Run inference\n",
    "results.show()  # Display image with detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71a22a-963b-432e-b8f6-bff40cf1b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture?\n",
    "import torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41a7a14-f465-4b23-b75f-a8cd1a54a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you perform inference on an online image using the Faster RCNN model and print the predictions?\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Load image from URL\n",
    "url = 'https://example.com/image.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Transform and inference\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7637d-c483-40dc-b4de-b243e15273fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you load an image and perform inference using YOLOv9, then display the detected objects with bounding boxes and class labels?\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # Replace with yolov9 if needed\n",
    "results = model('image.jpg')\n",
    "results[0].show()  # Show image with boxes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a62779-d5a6-45a5-98ac-18c4622204db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you display bounding boxes for the detected objects in an image using Faster RCNN?\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Assume `output` and `image` from Q3\n",
    "threshold = 0.5\n",
    "boxes = output[0]['boxes']\n",
    "scores = output[0]['scores']\n",
    "for box, score in zip(boxes, scores):\n",
    "    if score > threshold:\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4aabba-6729-4aa6-84a1-ee9f747291b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you perform inference on a local image using Faster RCNN?\n",
    "image = Image.open('local_image.jpg').convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89014cfc-aa58-4de2-919c-321fa604b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you change the confidence threshold for YOLO object detection and filter out low-confidence predictions?\n",
    "results = model('image.jpg', conf=0.6)  # Set confidence threshold to 0.6\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6762bfe-2030-43b3-a23a-13b5a3aacb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you plot the training and validation loss curves for model evaluation?\n",
    "If using YOLOv8/YOLOv9:\n",
    "tensorboard --logdir runs/train/exp\n",
    "Or using Matplotlib (assuming you have loss values stored):\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a030c17-e494-4b92-b3a2-e835ab20642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you perform inference on multiple images from a local folder using Faster RCNN and display the bounding boxes for each?\n",
    "import os\n",
    "folder_path = 'images/'\n",
    "for img_name in os.listdir(folder_path):\n",
    "    img = Image.open(os.path.join(folder_path, img_name)).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    # Display boxes (see Q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f9039-ca4d-40dc-af39-facc18a2a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster RCNN?\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box, score in zip(output[0]['boxes'], output[0]['scores']):\n",
    "    if score > 0.5:\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline='red', width=2)\n",
    "        draw.text((x1, y1), f'{score:.2f}', fill='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af114b6-c60c-468e-a0d6-efb1de1700f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can you save the inference results (with bounding boxes) as a new image after performing detection using YOLO?\n",
    "results = model('image.jpg')\n",
    "results[0].save(filename='output.jpg')  # Save image with detections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
