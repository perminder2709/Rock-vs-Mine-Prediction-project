{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e287ee-48be-46b5-a5b7-007f7c33f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is image segmentation, and why is it important?\n",
    "Image segmentation divides an image into regions or segments to simplify analysis. Each pixel is assigned a label corresponding to an object or region. It's important for:\n",
    "\n",
    "Precise object boundary detection.\n",
    "\n",
    "Applications like medical imaging, autonomous vehicles, and video surveillance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11aadab-478f-4229-8980-a0f33909219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain the difference between image classification, object detection, and image segmentation.\n",
    "Image Classification: Predicts a single label for the entire image (e.g., \"cat\").\n",
    "\n",
    "Object Detection: Identifies multiple objects with bounding boxes (e.g., \"cat\" at position X).\n",
    "\n",
    "Image Segmentation: Labels each pixel with its class, distinguishing object shapes and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fb0ee-7d14-4027-bea7-ea2293ff09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is Mask R-CNN, and how is it different from traditional object detection models?\n",
    "Mask R-CNN extends Faster R-CNN by adding a mask prediction branch for instance segmentation. While traditional object detectors only provide bounding boxes, Mask R-CNN outputs:\n",
    "\n",
    "Bounding boxes.\n",
    "\n",
    "Class labels.\n",
    "\n",
    "Pixel-level masks for each detected object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50350c9-4233-4953-812c-a64be68741bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What role does the \"RoIAlign\" layer play in Mask R-CNN?\n",
    "RoIAlign ensures precise spatial alignment of extracted features by avoiding quantization errors. It interpolates features for region proposals, improving mask prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467300a-40f0-4d09-a8a9-3430c000a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are semantic, instance, and panoptic segmentation?\n",
    "Semantic Segmentation: Labels each pixel with a class (e.g., \"road,\" \"car\").\n",
    "\n",
    "Instance Segmentation: Differentiates instances of the same class (e.g., \"car 1,\" \"car 2\").\n",
    "\n",
    "Panoptic Segmentation: Combines semantic and instance segmentation for holistic scene understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdc6d2-0608-4319-bd8c-809d7b208f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the role of bounding boxes and masks in image segmentation models.\n",
    "Bounding Boxes: Coarse localization of objects.\n",
    "\n",
    "Masks: Precise delineation of object boundaries at the pixel level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546e9a6-c45d-4e22-943a-b81a4052d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the purpose of data annotation in image segmentation?\n",
    "Data annotation provides pixel-level labels for supervised learning. High-quality annotations ensure:\n",
    "\n",
    "Better model training.\n",
    "\n",
    "Accurate object detection and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8de1d-6f29-490c-b9b5-4d5d3212cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does Detectron2 simplify model training for object detection and segmentation tasks?\n",
    "Detectron2 simplifies training by:\n",
    "\n",
    "Offering pre-trained models.\n",
    "\n",
    "Supporting COCO-format datasets and custom data pipelines.\n",
    "\n",
    "Providing tools for visualization and evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205f051-d327-402a-a0bd-6cf0cce906ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why is transfer learning valuable in training segmentation models?\n",
    "Transfer learning:\n",
    "\n",
    "Uses pre-trained weights for faster convergence.\n",
    "\n",
    "Requires fewer labeled data.\n",
    "\n",
    "Improves performance on small or custom datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f3ee0-6f09-4b8e-9bc3-407c2ec6ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does Mask R-CNN improve upon the Faster R-CNN model architecture?\n",
    "Mask R-CNN adds a branch to Faster R-CNN for mask prediction. Key improvements:\n",
    "\n",
    "Pixel-level precision with instance masks.\n",
    "\n",
    "Use of RoIAlign for better feature alignment.\n",
    "\n",
    "Enhanced multi-task learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b296c-ce02-4686-afee-1b69575ecfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is meant by \"from bounding box to polygon masks\" in image segmentation?\n",
    "It refers to the transition from rough object localization (bounding boxes) to accurate pixel-level delineation using polygonal or binary masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7adab-00a3-4059-a7e4-64ada1cc97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does data augmentation benefit image segmentation model training?\n",
    "Data augmentation:\n",
    "\n",
    "Increases dataset size artificially.\n",
    "\n",
    "Reduces overfitting.\n",
    "\n",
    "Improves model robustness to variations in input (e.g., lighting, scale, rotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f03509-5c5b-41f0-84e4-365e991931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the architecture of Mask R-CNN, focusing on the backbone, region proposal network (RPN), and segmentation mask head.\n",
    "Backbone: Extracts features (e.g., ResNet with FPN for multi-scale features).\n",
    "\n",
    "RPN: Proposes candidate object regions.\n",
    "\n",
    "Mask Head: Predicts binary masks for each object in the region proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc694c-fbaa-4f2c-826b-2ce2b17237a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain the process of registering a custom dataset in Detectron2 for model training.\n",
    "Prepare Data: Convert annotations to COCO format.\n",
    "\n",
    "Register Dataset:\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "def load_dataset():\n",
    "    return your_dataset_dict\n",
    "DatasetCatalog.register(\"dataset_name\", load_dataset)\n",
    "MetadataCatalog.get(\"dataset_name\").set(thing_classes=[\"class1\", \"class2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb910f5c-e89e-4066-8cb5-76b1b543af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What challenges arise in scene understanding for image segmentation, and how can Mask R-CNN address them?\n",
    "Challenges:\n",
    "\n",
    "Occlusions and overlapping objects.\n",
    "\n",
    "Complex object boundaries.\n",
    "Mask R-CNN addresses these by:\n",
    "\n",
    "Predicting instance-specific masks.\n",
    "\n",
    "Using RoIAlign for better feature alignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d2c64-039e-4747-8fb2-b4c796700129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discuss the use of transfer learning in Mask R-CNN for improving segmentation on custom datasets.\n",
    "Transfer learning involves fine-tuning a pre-trained Mask R-CNN model. Benefits:\n",
    "\n",
    "Reduces training time.\n",
    "\n",
    "Leverages generalized feature extraction from large datasets.\n",
    "\n",
    "Enhances performance on limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885905c2-b28f-4fc5-a952-8f070e7d2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the purpose of evaluation curves, such as precision-recall curves, in segmentation model assessment?\n",
    "Precision-recall curves:\n",
    "\n",
    "Evaluate trade-offs between precision and recall.\n",
    "\n",
    "Help choose optimal confidence thresholds.\n",
    "\n",
    "Provide insights into model performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa4c5-9088-473d-8fd0-991f09f6bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do Mask R-CNN models handle occlusions or overlapping objects in segmentation?\n",
    "Mask R-CNN handles occlusions by:\n",
    "\n",
    "Predicting masks independently for each object.\n",
    "\n",
    "Using non-maximum suppression to refine overlapping detections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b7b11-f096-4f22-a033-6e78d44c0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain the impact of batch size and learning rate on Mask R-CNN model training.\n",
    "Batch Size: Larger batches stabilize gradients but require more memory.\n",
    "\n",
    "Learning Rate: Affects convergence; high rates may cause divergence, while low rates slow training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90e5ba-fb8f-4a78-a3ba-c9e079fc4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the challenges of training segmentation models on custom datasets, particularly in the context of Detectron2.\n",
    "Challenges:\n",
    "\n",
    "Annotation quality.\n",
    "\n",
    "Imbalanced datasets (e.g., underrepresented classes).\n",
    "\n",
    "Dataset format compatibility (e.g., COCO format).\n",
    "Detectron2 addresses these with its robust dataset handling and augmentation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0d698-8822-4575-9e4b-8fc7291af51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does Mask R-CNN's segmentation head output differ from a traditional object detector's output?\n",
    "Mask R-CNN outputs:\n",
    "\n",
    "Bounding Boxes: Object localization.\n",
    "\n",
    "Class Scores: Object classification.\n",
    "\n",
    "Segmentation Masks: Pixel-level delineation for each object.\n",
    "Traditional object detectors lack pixel-level segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f8dff-4ee8-4ae0-b105-33bd9b5cc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform basic color-based segmentation to separate the blue color in an image.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"image.jpg\")\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the range of blue color in HSV\n",
    "lower_blue = np.array([100, 150, 0])\n",
    "upper_blue = np.array([140, 255, 255])\n",
    "\n",
    "# Create a mask for blue color\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "# Apply the mask to the image\n",
    "blue_segment = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"blue_segment.jpg\", blue_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e31542-3560-45af-bd93-2693cf772d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use edge detection with Canny to highlight object edges in an image.\n",
    "# Load the image in grayscale\n",
    "gray = cv2.imread(\"image.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(gray, threshold1=50, threshold2=150)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"edges.jpg\", edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff540a-ca1f-4168-9a97-951c5f2a411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a pretrained Mask R-CNN model from PyTorch and use it for object detection and segmentation on an image.\n",
    "\n",
    "import torch\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pretrained Mask R-CNN model\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(\"image.jpg\").convert(\"RGB\")\n",
    "tensor_image = F.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(tensor_image)\n",
    "\n",
    "# Extract masks, boxes, and labels\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16142b04-fd3f-4a02-95d4-a7933db0cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate bounding boxes for each object detected by Mask R-CNN in an image.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def draw_bounding_boxes(\n",
    "    image: np.ndarray, \n",
    "    outputs: List[Dict[str, Any]], \n",
    "    confidence_threshold: float = 0.5,\n",
    "    figsize: Tuple[int, int] = (12, 9),\n",
    "    box_color: str = 'r'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes for objects detected by Mask R-CNN on an image.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): The input image to draw bounding boxes on\n",
    "        outputs (List[Dict[str, Any]]): Detection outputs from Mask R-CNN model\n",
    "        confidence_threshold (float, optional): Minimum confidence score to display a box. Defaults to 0.5.\n",
    "        figsize (Tuple[int, int], optional): Figure size for the plot. Defaults to (12, 9).\n",
    "        box_color (str, optional): Color of the bounding box. Defaults to 'r'.\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays the image with bounding boxes\n",
    "    \"\"\"\n",
    "    # Extract bounding boxes and scores\n",
    "    boxes = outputs[0]['boxes'].numpy()\n",
    "    scores = outputs[0]['scores'].numpy()\n",
    "\n",
    "    # Create figure and display image\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bounding boxes for detections above threshold\n",
    "    for box, score in zip(boxes, scores):\n",
    "        if score > confidence_threshold:\n",
    "            # Calculate width and height from box coordinates\n",
    "            width = box[2] - box[0]\n",
    "            height = box[3] - box[1]\n",
    "            \n",
    "            # Create and add rectangle\n",
    "            rect = Rectangle(\n",
    "                (box[0], box[1]), width, height,\n",
    "                linewidth=2, edgecolor=box_color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# draw_bounding_boxes(image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43746016-c9a0-4c96-8e9d-265bc45fa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert an image to grayscale and apply Otsu's thresholding method for segmentation.\n",
    "\n",
    "# Load the image in grayscale\n",
    "gray = cv2.imread(\"image.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Otsu's thresholding\n",
    "_, otsu_threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"otsu_threshold.jpg\", otsu_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29338784-5757-48b3-a4fd-bf6f8b77f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform contour detection in an image to detect distinct objects or shapes.\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(otsu_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on the original image\n",
    "contour_image = cv2.drawContours(image.copy(), contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"contour_image.jpg\", contour_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253df5c-8476-4a68-b52e-86180aed8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Mask R-CNN to detect objects and their segmentation masks in a custom image and display them.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Extract masks and overlay them on the image\n",
    "masks = outputs[0]['masks'].numpy()\n",
    "\n",
    "# Combine all masks\n",
    "final_mask = np.zeros_like(masks[0][0], dtype=np.uint8)\n",
    "for i, mask in enumerate(masks):\n",
    "    if scores[i] > 0.5:  # Confidence threshold\n",
    "        final_mask = np.maximum(final_mask, mask[0] > 0.5)\n",
    "\n",
    "# Overlay the mask on the original image\n",
    "masked_image = cv2.addWeighted(image, 0.7, final_mask[:, :, None] * 255, 0.3, 0)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"masked_image.jpg\", masked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04200c4-529f-4ac8-8436-e65841418853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply k-means clustering for segmenting regions in an image.\n",
    "\n",
    "# Reshape the image to a 2D array of pixels\n",
    "pixel_values = image.reshape((-1, 3))\n",
    "pixel_values = np.float32(pixel_values)\n",
    "\n",
    "# Define criteria and number of clusters\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "k = 3  # Number of clusters\n",
    "\n",
    "# Apply k-means clustering\n",
    "_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Convert back to uint8 and reshape to original image shape\n",
    "centers = np.uint8(centers)\n",
    "segmented_image = centers[labels.flatten()].reshape(image.shape)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite(\"segmented_image.jpg\", segmented_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
